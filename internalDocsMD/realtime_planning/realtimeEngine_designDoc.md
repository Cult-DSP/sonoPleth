Hard-Realtime Spatial Audio Engine Design Document
Introduction and Goals

Designing a hard-realtime spatial audio engine requires balancing high channel counts, low latency, and reliable performance. This document presents the architecture for a realtime engine capable of playing up to 128 simultaneous mono PCM streams, spatialized over an arbitrary multichannel loudspeaker array using Distance-Based Amplitude Panning (DBAP). The engine is optimized for immersive installations (e.g. the AlloSphere) and runs under strict realtime constraints: no file I/O, dynamic memory allocation, or locks are permitted in the audio callback. All processing must complete within each audio frame bufferâ€™s deadline to avoid glitches, as even a single drop-out or click in live performance can undermine user confidence. Key goals include:

Accurate Spatialization: Use DBAP to pan sources based on true speaker positions, with runtime control of focus (source spread) and elevation mode. The design preserves the offline rendererâ€™s spatial semantics (block-based motion, vertical mapping modes) for consistency (see RENDERING.md for offline definitions).

Scalability: Support up to 128 sources concurrently (e.g. dozens of dynamic â€œaudio_objectsâ€ plus static bed channels) and high output channel counts (50+ speakers plus subwoofers), while maintaining real-time performance on modern multi-core machines (e.g. desktop i9 or MacBook Pro).

Robust Streaming: Stream large audio files from disk without loading into RAM, using double-buffering and background threads to prevent disk reads from blocking the audio thread. Both standard WAV and RF64 (for >4GB files) are supported, addressing the 4GB RIFF limit.

Hard-Realtime Safety: Ensure the audio callback never calls into the OS or performs unbounded work. All slow operations (file I/O, decompression, memory allocation, locks, etc.) are confined to dedicated threads or pre-computed before playback. This adheres to known best practices for glitch-free audio: â€œDonâ€™t hold locksâ€¦ donâ€™t allocate memoryâ€¦ donâ€™t do file or network I/O on the audio thread.â€

Flexible Output & Monitoring: Provide a final output routing stage for mapping logical speaker outputs to physical device channels (with one-to-one mapping and no duplicates, per channelMapping.hpp). Include a master gain and per-bus trim (e.g. a speaker bus compensation slider) to calibrate overall levels. Basic metering and diagnostics can be integrated (following patterns from mainplayer.cpp for level monitoring), though these run on auxiliary threads to avoid audio thread overhead.

Fail-Safe Performance: Incorporate a CPU safety manager that monitors audio callback load and dynamically degrades processing quality if needed (e.g. reduces spatial update rate, as described below). Under worst-case load, the engine will gracefully trade off spatial precision to preserve uninterrupted audio output (no buffer underruns). This design philosophy aligns with voice management in pro audio engines, where lower-priority sounds are culled or simplified when resources are strained.

By meeting these goals, the engine will provide reliable, sample-accurate spatial audio playback for complex scenes, suitable for both research and artistic applications. In the following sections, we detail the technical background of DBAP, then dive into the engineâ€™s architecture: streaming, spatialization, thread model, control logic, and output handling. An appendix compares possible audio backends (AlloLib, PortAudio, JACK) and outlines future extensions (such as calibration and advanced voice management).

Background: Distance-Based Amplitude Panning (DBAP)

Distance-Based Amplitude Panning (DBAP) is a channel-based spatialization technique that localizes sound by distributing gains to speakers according to their distances from the virtual source. Unlike traditional panning methods (stereo, VBAP, ambisonics) that assume a listener in the â€œsweet spotâ€ center, DBAP makes no assumptions about listener position and places no restrictions on speaker layout. All speakers contribute to the sound at all times, and the total power remains approximately constant regardless of source location. This makes DBAP well-suited for irregular arrays and large installations where listeners move around or are not centered.

In DBAP, each speaker is assigned a gain weight $v_i$ based on the inverse-distance from the source to that speaker. A typical formulation is:

ð‘£
ð‘–
=
ð‘˜
ð‘‘
ð‘–
ð‘Ž
,
v
i
â€‹

=
d
i
a
â€‹

k
â€‹

,

where $d_i$ is the distance from the source to speaker $i$, and the exponent $a$ controls the rolloff of gain with distance. The normalization factor $k$ is chosen so that the sum of power over all channels is 1 (ensuring constant total intensity). In practice, $a$ corresponds to a rolloff in dB per distance doubling (for example, $a=1$ with a 6 dB per doubling yields an inverse-distance law). Lower values of $a$ produce a gentler rolloff (wider spatial spread), while higher values focus more energy on the nearest speakers. Our engine exposes this as a â€œfocusâ€ parameter (range ~0.2 to 5.0, default 1.0), similar to the offline rendererâ€™s --dbap_focus option (see RENDERING.md). A low focus (e.g. 0.5) causes a source to engage many speakers with little level difference, creating a diffuse image. A high focus (e.g. 3.0) causes rapid attenuation with distance, so only speakers very close to the source get significant gain (a tighter, more localized image).

One challenge with DBAP is when a virtual source comes extremely close to a particular speaker. In the pure formula, as distance $d_j \to 0$ for speaker $j$, its gain $v_j$ would approach 1 while all others approach 0, essentially collapsing the source to a single speaker. This can cause a noticeable change in timbre and â€œspatial blurâ€ as the source transitions to or from that state. To mitigate this, the original DBAP paper suggests adding a small constant â€œblur radiusâ€ to distances, preventing any speaker from ever reaching a gain of 1. Our design plans a speaker compensation mechanism at the mixing stage to address this issue (see Speaker Compensation in Spatialization module). In practice, this means slightly reducing the gain disparity when a source is extremely close to one speaker, preserving a bit of spread to avoid abrupt changes in tone. The engineâ€™s manual speaker-bus trim control (Â±5 dB) can also serve to fine-tune overall balance between direct speaker outputs and the rest of the mix, which can help subjectively â€œsmoothâ€ the image when using very high focus values (this is a stop-gap for v1, with more precise calibration planned in future versions).

DBAPâ€™s strength is its flexibility: it works with any loudspeaker arrangement and yields smooth, continuous pans without silent zones. The cost is that all speakers are technically active at once, which for large arrays means many small gains to compute and apply. However, because distant speakersâ€™ contributions will be very low, an implementation can set a threshold to skip negligible gains or use the focus parameter to effectively limit how many speakers are significantly active per source. (A possible extension is a K-nearest approach where only the closest K speakers get nonzero gain, similar to KNN algorithms, but our default DBAP keeps the classical all-speaker approach for accuracy.) In summary, DBAP provides a robust spatial foundation for the engine, ensuring that as sound sources move throughout a 3D space, their signals are appropriately distributed across the available speakers with consistent energy and without relying on a fixed listening spot.

System Architecture Overview

The engine is structured as a set of cooperating modules that handle streaming, spatialization, mixing, and output (see also the offline pipeline in RENDERING.md for analogous components). Below is a high-level data flow:

Disk Streaming Module â€“ Reads mono audio files (â€œstemsâ€) from disk in real time, feeding a buffer of audio frames for each active source.

Pose & Control Module â€“ Receives source trajectories and parameters (from a pre-parsed LUSID scene) and provides the current position/orientation of each source, updated at the start of each audio block. Also handles user/runtime parameter changes (focus, elevation mode, mutes, etc.).

Spatialization (DBAP) Mixer â€“ Computes per-source, per-speaker gain matrices based on source positions and the selected spatialization settings. Applies gains to each sourceâ€™s audio and accumulates signals into each speaker channel (summing all sources).

LFE Routing â€“ Separates any LFE (low-frequency effects) channels or sources and routes them directly to subwoofer output channels, bypassing the spatial panner. LFE signals are optionally split across multiple subs to distribute power.

Speaker Compensation & Master Gain â€“ Applies any global adjustments: e.g. a small gain tweak on the overall speaker bus (excluding subs) for calibration, and a master gain to prevent clipping or adjust overall volume. These operate at the end of the mix.

Output Remapping Module â€“ Maps the engineâ€™s logical output channels to the physical audio interface channels. This allows arbitrary reordering or exclusion of channels, subject to a one-to-one mapping (each engine channel maps to a unique device channel). The final audio is then delivered to the selected audio backend (AlloLib, PortAudio, JACK, etc.) for playback.

All modules are engineered for hard-realtime: the audio thread runs the Spatialization, LFE, compensation, and mixing steps every audio frame. Expensive tasks like disk I/O or JSON parsing occur in other threads before or alongside the audio thread, with thread-safe handoff of data. Below, we examine each module in detail.

Streaming Module (Disk I/O and Buffering)

Responsibility: Stream audio from disk for each source without blocking the audio callback. This module handles reading PCM data from WAV/RF64 files and feeding it to the rest of the engine in time. Given that multi-channel scene audio can be huge (multi-gigabyte files), we cannot pre-load all audio into memory. Instead, we implement a double-buffering strategy with a dedicated loader thread (following the prototype in streamingWAV.md).

In this design, each audio source has two buffers that alternate roles: one buffer is actively being played by the audio thread, while the other is being filled with the next segment of audio by the loader thread. Each buffer might contain, for example, a few seconds of audio frames for that source (the optimal chunk size is a tunable parameter â€“ larger chunks reduce disk seeks but use more memory). When the playback position nears the end of the current buffer, the loader thread is signaled (via an atomic flag) to start reading the next chunk of data into the idle buffer. The read uses block-aligned I/O (e.g. reading in blocks of 512 frames) to avoid partial frame reads and to cooperate with typical disk/cache behavior. By the time the audio thread finishes playing the current buffer, the next buffer is ready (ideally), and the module atomically switches the active buffer pointer. This lock-free buffer swap ensures the audio thread never waits on the loader. If the loader falls behind and the next buffer isnâ€™t ready in time (an unlikely case if tuned properly), the engine has a fallback: it can perform a direct disk read on the audio thread as a last resort. This â€œsafety netâ€ ensures continuity but is highly undesirable since disk I/O on the audio thread risks a dropout. Thus, chunk size and preload thresholds are configured conservatively (e.g. start loading a new chunk when the current chunk is only 25â€“50% consumed) to give ample time for background loading.

Memory allocation: All stream buffers are pre-allocated to the maximum chunk size for each source at load time, so the audio thread only ever reads from an existing array (no malloc/free during playback). The double buffers and state flags are encapsulated in a small struct per source, with atomics (or lock-free structures) to coordinate state (e.g. flags like READY/PLAYING for each buffer). A global frameCounter (sample position) is maintained per source or globally to know what chunk to load next. The loader thread uses a mutex only around file access (seeking and reading), since the underlying file I/O library (e.g. libsndfile or similar via Gamma SoundFile) is not inherently thread-safe. This mutex is not used by the audio thread (the audio thread never reads from file, only from buffers), so it does not violate realtime safety.

RF64 and large file support: The engine detects standard WAV vs RF64. WAV files are limited to 4GB by a 32-bit size field. Our offline tools already handle auto-switching to RF64 when output files exceed this size. In playback, we rely on libsndfile (or an equivalent) to transparently open RF64 files, so the same streaming logic applies. The important consideration is ensuring that file offsets and chunk calculations use 64-bit integers, which our implementation does (using uint64_t for frame indices and sizes). This allows extremely long duration audio streams (many hours) if needed. We also note that some older or simpler WAV readers might misreport lengths for RF64, but by using a well-tested library and format schema, we mitigate that risk.

Thread and Buffer Configuration: Through experimentation and reference designs, typical buffer sizes in realtime audio are 256 to 1024 frames per block for comfortable safety, but to minimize latency we often use smaller blocks (e.g. 64 or 128 frames). Our streaming chunks, however, are much larger (on the order of tens of thousands of frames or more) to amortize disk access. For example, the prototype uses a chunk of 1 minute of audio at 48 kHz (~2.88 million frames). This is about 5.76 MB for a mono source (double that for stereo, etc.), which is a reasonable memory tradeoff. We may use smaller chunk sizes if many sources are streaming to avoid memory exhaustion, but given target machines have ample RAM, a few hundred MB total buffering for dozens of sources is acceptable. We can also dynamically adjust chunk sizes or preload more aggressively if disk latency spikes. The streaming moduleâ€™s architecture is designed such that the audio callback never waits on disk I/O â€“ it either has data in the current buffer or, in worst case, performs an immediate read of a small missing segment (which could glitch, but prevents a prolonged gap). By adhering to this double-buffer pattern, we satisfy realtime constraints and ensure smooth playback of large multichannel files.

Pose and Control Module (Time & Parameter Management)

Responsibility: This module bridges the input scene description (e.g. the LUSID scene JSON) and runtime controls to the audio rendering system. It provides the timebase and source state for each audio block. A global sample-accurate frame counter is used as the master timeline; this is essentially the number of audio samples since playback start (or since a loop restart). Using a frame counter ensures time alignment between audio and control â€“ for example, if a source should start at 10.0 seconds, we can calculate the exact sample index (10.0 \* 48000) and begin mixing only at that frame. The frame counter increments by framesPerBuffer on each callback (with adjustments if the last block is a partial). We convert this to seconds or other units only for external reporting; internally all scheduling (e.g. keyframe lookup) is done in integer samples to avoid drifting or rounding issues.

At the start of each audio block, the Pose module updates each sourceâ€™s parameters for that blockâ€™s timeframe. This involves:

Determining which sources are active during this block. The engine uses metadata from upstream (the scene parser) such as a â€œcontainsAudioâ€ mask or explicit active flags to skip processing silent or inactive sources. Sources that are inactive (e.g. before their start time or after their end, or muted by user) are marked and the mixer will bypass them entirely (saving CPU). We do not destroy or reallocate source slots on the fly; instead we maintain a fixed array of source slots (size 128) and simply mark inactive ones (this avoids reallocations and keeps source indices stable, as noted in the internal design notes â€“ see AGENTS.md).

For each active source, retrieving its intended position and orientation at the current time. The LUSID scene data provides time-keyed trajectories for moving objects (type "audio_object") and fixed coordinates for static ones (type "direct_speaker" for bed channels). We use block-aligned timing: typically, we take the source position at the center of the upcoming audio block as representative. This means if the block spans time 5.000s to 5.021s, we sample the trajectory at ~5.0105s (center) for panning calculation. This technique, also used in our offline renderer (see SpatialRenderer.cpp for interpolation logic), smooths motion and ensures consistency between realtime and offline results. If needed, linear interpolation between keyframes is done to get the exact position. Because our block sizes are small (e.g. 64 samples = 1.33ms), this approximation introduces negligible spatial error and is far more efficient than recomputing gains every single sample.

Handling elevation mode and coordinate transforms. The engine supports multiple elevation mapping modes to accommodate different 3D speaker distributions (e.g., â€œRescaleAtmosUpâ€, â€œRescaleFullSphereâ€, â€œClampâ€, etc., as referenced in RENDERING.md). These modes decide how to treat the vertical (Z) component of source position: for instance, Atmos-style might compress elevations so that extreme heights are reduced (if the speaker layout has no speakers directly above, etc.), or a clamp mode might limit negative elevations. The Pose module applies these transforms to the source coordinates before passing them to the spatializer. Internally, the offline SpatialRenderer::directionToDBAPPosition() applied a coordinate axis swap for AlloLibâ€™s DBAP implementation â€“ our engine will include any analogous transforms so that the positions align with the spatializerâ€™s expected coordinate system (see SpatialRenderer.cpp for the current approach, which for AlloLib flips the Y/Z axes). The elevation mode is a runtime toggle (can be changed on the fly by the user), but such changes only take effect on block boundaries. This is acceptable since sudden mid-block changes in elevation processing could cause an audible jump; deferring to the next buffer ensures a clean switch.

Processing runtime commands: e.g., global master gain changes, focus adjustments, solo/mute toggles, or loop/seek events. The control module receives these (likely from a UI or network thread) and uses lock-free structures or atomic flags to communicate them to the audio thread. A typical pattern is double-buffering the control state: the main thread writes new parameter values into a control block and flips an atomic flag, and the audio thread, at safe points (block start), will read and apply those changes. This avoids using any locks on the audio side. For example, if the user changes focus = 2.0, we update an atomic pendingFocus=2.0. The audio thread, before computing new gains for the block, will read that and update the effective focus parameter used in calculations. By design, all such control changes apply quantized to the block boundary â€“ this yields a control rate of (block_rate) ~ 700â€“1500 Hz, which is more than sufficient for smooth parameter transitions. In cases where a parameter change must be smoothed (e.g. master gain change to avoid a click), we implement a ramp (see Gain Smoothing below).

This Pose/Control stage essentially sets up all the static data the audio thread will need for the upcoming block: which sources are on, their positions (already transformed if needed), and the current values of spatialization parameters. It thus acts as a snapshot of the world at a given time. The separation of this stage into a non-audio thread (or at least a non-time-critical portion) means we can afford minor overhead here (like parsing JSON for a new keyframe or handling user input) without risking the audio deadline. The result is then used by the next stage, the spatial mixer.

Spatialization Mixer (DBAP Gain Calculator and Mixer)

Responsibility: This is the core audio mixing loop that applies the DBAP algorithm. For each active source and each speaker channel, we calculate a gain and apply the sourceâ€™s audio to that speakerâ€™s buffer. Conceptually, itâ€™s a matrix multiplication: an N_source Ã— M_speaker gain matrix is applied to N_source audio signals, producing M_speaker outputs.

To meet realtime performance, we exploit the block processing nature and any sparsity we can:

Block-rate gain computation: Using the position provided for the block (from Pose module), we compute the gain weights $v_{i,j}$ for source i into speaker j once per block, not per sample. These gains are then used for the entire blockâ€™s samples for that source. This is valid because source movement within a ~1ms block is minimal. If a source moves fast, we might incorporate a linear crossfade between the gain at block start vs block end, but currently we assume block-center as representative (which matches our offline renderer approach for simplicity). Computing gains per block drastically cuts down CPU usage compared to per-sample calculation.

Vectorized mixing: After obtaining all gains for the current block, the engine multiplies and accumulates audio samples in an optimized loop. Modern CPUs allow SIMD operations (e.g. processing 8 samples at once with AVX instructions). We plan to leverage this by processing each sourceâ€™s audio frames in chunks and using vector multiply-add operations to apply gain and add to speaker sums. The inner loop effectively is: for each source, for each speaker: speakerBuffer[j][frame..frame+7] += sourceBuffer[frame..frame+7] \* gain[i,j] (with appropriate vector size). The order of loops can be arranged to favor cache locality (e.g. iterate frames in the innermost loop).

Skipping negligible gains: With DBAP, some gains will be very small (if a source is far from a speaker). We can set a threshold (say -60 dB) below which we treat the gain as zero and skip that contribution. This can substantially reduce work when focus is high or the source is localized â€“ effectively only nearby speakers are significant. We maintain a list of â€œactive speakersâ€ for each source if possible, updated each block. For example, if focus=5.0 (very tight), maybe only 3-5 speakers have non-trivial gains, so we only loop over those for mixing that source. In the worst case (focus=1.0 default), many speakers are active but itâ€™s still distributed. This optimization will be considered in v1 if needed for CPU, but even without it the engine can handle moderate source counts by virtue of block processing and modern CPU vectorization.

Direct speaker bypass: Some sources are of type direct_speaker â€“ meaning they correspond to a fixed speaker feed (like a channel-based bed). In principle, we could bypass the panning computation for these and directly route the source to its designated speaker output (since a direct speaker should ideally map 1:1 to a physical speaker). However, our implementation treats direct_speaker as a special case of an audio object that doesnâ€™t move (static position equal to that speakerâ€™s coordinates). The DBAP calculator will handle it naturally: if a source is exactly at a speakerâ€™s location, the gains outcome (with a bit of blur) will be that mostly that speaker gets the signal. For efficiency, we still short-circuit: the engine can detect node.type == direct_speaker and simply copy that sourceâ€™s audio to the corresponding output channel (with appropriate gain scaling and master gain) rather than iterating through all speakers. This is an optional optimization to reduce overhead for bed channels, and it aligns with treating direct speakers effectively as static routes (indeed, in AGENTS.md, direct_speakers are treated as static audio_objects for semantics). Even if we use the general mixer for them, it wonâ€™t be too costly because one source with a focused position at a speaker will have negligible gains to others (which can be skipped).

The gain computation itself uses the DBAP formula discussed earlier. We incorporate the focus exponent: effectively, if focusExponent = a, we use $d_i^a$ in the denominator. The code structure (mirroring offline SpatialRenderer.cpp) is roughly: for each source i, for each speaker j, compute distance d*j = || position_i - speakerPos_j || (with maybe an added minimum distance or blur radius), then compute preliminary gain = 1 / (d_j^a). Then normalize across all speakers j for that source so that $\sum_j v*{i,j}^2 = 1$ (constant power). We also handle edge cases: if a source lies outside the convex hull of speakers, extremely far, or extremely close. If outside the convex hull (sound outside the array), DBAP tends to equalize all gains (all speakers become almost equally loud as distance increases). This can reduce directional localization. A possible enhancement (not in v1) is to project sources outside the hull onto the hull boundary for panning purposes, to avoid that collapse â€“ but we mention it here as a known consideration. For v1, we assume most sources of interest are within or around the speaker array. If a source exactly coincides with a speaker position, our blur compensation (adding a tiny $r_s$ or minimum distance) prevents division by zero and avoids a single-speaker takeover.

After computing and applying gains, the mixer accumulates all sources into the output speaker buffers. By the end of this stage, we have a set of M speaker channel buffers, each containing the sum of all active sources appropriately weighted.

Gain Smoothing: To avoid clicks or pops when sources start/stop or when gains change abruptly, the engine applies gain ramping at the block boundaries. If a source becomes active or inactive, we donâ€™t snap the gain matrix from all zeros to nonzero in one sample â€“ instead we can fade in over a short interval (for example, 64 samples = ~1.3ms, which is typically inaudible). Similarly, if the focus parameter or a sourceâ€™s position jumps (perhaps due to a sudden user action), we can interpolate gain values from the last block to the new block over the first few samples. In practice, because our block size is small, many parameter changes will already be minor per block. But for cautious design, we linearly ramp any large gain change. For example, the muting of a source can be implemented as a 0.5ms fade-out rather than an immediate cut. These tiny fades ensure no wideband transient (click) is introduced. Modern DAWs and game engines use similar tactics (e.g. a 64-sample crossfade on voice start/stop is common). We incorporate this by storing the previous blockâ€™s gain values and, when a change exceeds a threshold, applying an envelope to the first few samples of the block. This is done per source-per speaker gain if needed, but usually a simpler per-source ramp (fade in/out the whole source) suffices when a source appears or disappears.

Summing up, the spatial mixer is where the heavy DSP math happens, but through block-based calculation, smart skipping, and avoiding unnecessary work, it fits within realtime budgets. The design target (128 sources Ã— ~60 speakers) is ambitious but achievable on a high-end CPU, especially since typically not all 128 will be active or widely spread at once. Additionally, the CPU Safety measures (discussed later) can dynamically reduce the load here if we approach the time limit.

LFE Handling (Subwoofer Routing)

Responsibility: Handle low-frequency effect channels separately from spatial panning. In many spatial audio scenes (like Dolby Atmos bed+objects), an LFE channel is included for deep bass content that is not intended to be directional. In our engine, any source designated as type "LFE" (as per the scene schema) is routed directly to the subwoofer outputs (type "LFE" nodes are identified and not processed by DBAP).

The engine configuration will know how many subwoofer channels the output layout has (e.g. in the AlloSphere, perhaps 4 or 6 subs distributed around). During initialization, we determine the index of those subs (either via the layout file or a convention â€“ e.g. certain speaker IDs are marked as subs). The Output channel count is set to accommodate the highest-index speaker or sub; for example, if the highest speaker ID is 59 and we have subs at 60-61, then we allocate 62 output channels (0â€“61).

For an LFE source (which is typically a mono .wav containing the LFE track of a mix), the engine will bypass the spatializer and instead copy that audio to the subwoofer channels. If multiple sub channels exist, we distribute the signal among them. The simplest approach is to send the identical LFE signal to all subs at a reduced gain such that the combined energy is correct. For instance, with two subs, each could get the LFE at â€“3 dB (1/âˆš2 amplitude) so that together they sum to the original level. With four subs, each gets â€“6 dB, and so on (i.e. divide by number of subs). This ensures that no matter how many subs are configured, an LFE source contributes the same total sound power. This division assumes all subs are intended to reproduce the LFE in unison (which is common to fill the room evenly with bass).

This routing is done in the audio thread but is trivial computationally: itâ€™s just copying a buffer and scaling it. We still treat LFE sources as separate from normal sources in the code â€“ for example, the Pose module will mark LFE sources and the Spatializer will ignore them (or we route them in a separate branch). Conceptually, one can think of an LFE source as feeding a dedicated subwoofer bus. Our engine sums all such LFE sources into a subwoofer bus, which has one channel per sub output. (If the content has only one LFE track, it goes to all subs as described. If there were multiple independent LFE channels â€“ unusual but possible â€“ each could be routed similarly.)

One caution: Because LFE content can be high amplitude (bass-heavy explosions, etc.), splitting it across subs means each sub gets a fraction. But if the system expects each subwoofer to get a specific LFE channel, our approach effectively turns them into a mono LFE distributed. In the AlloSphere context, likely all subs share the same LFE signal for redundancy and even coverage, so this is appropriate. We ensure that the remapping stage (next section) accounts for sub channels as well â€“ i.e. the subs will also be subject to output channel mapping if needed, but usually subs are fixed in hardware mapping.

Finally, we do not apply the master spatializer gain or focus to LFE channels. LFE is not spatialized, and it should not be affected by focus (since focus pertains to directional spread). We do apply master gain to LFE as part of the overall output, since itâ€™s a global volume control (the user likely expects the master gain knob to affect all sound including LFE). Also, the speaker compensation slider in v1 is defined to be â€œspeaker-bus onlyâ€, which implies it does not affect LFE bus. This allows independent tuning of main speakers vs subs. For example, if the user finds the bass too high relative to main mix, they wouldnâ€™t use the speaker compensation for that (they would directly adjust LFE gain or design the content differently). The compensation slider is intended for the mains (speakers) only, so it leaves LFE channel levels unchanged.

In summary, LFE handling in the engine is straightforward but important for faithful playback of content with dedicated bass channels. By segregating LFE early, we keep the spatializer focused on mid/high frequency content and avoid feeding low-frequency sounds into speakers that may not handle them or where itâ€™s not intended. Each LFE source is efficiently routed to subwoofers, maintaining energy calibration across however many subs are present.

Speaker Compensation and Gain Smoothing

After the spatial mixer and LFE routing, we have preliminary signals for each output channel (speakers 0â€¦N-1, and possibly subs). Two final adjustments are applied at this stage: speaker compensation and master gain / smoothing.

Speaker Compensation: This refers to a small corrective gain applied to the speaker channels to account for system-specific response or focusing issues. In version 1, we implement this as a simple user-controlled trim on the entire speaker bus (all main speakers together) ranging from -5 dB to +5 dB. The intent is to let the operator subtly adjust the overall loudness of the directional speakers relative to the subwoofers or to compensate for any perceptual effects of extreme focusing. For example, if running with a very high focus value, sources can sound too isolated on single speakers; reducing the speaker bus level a couple dB might blend in more room effect or make the overall image less harsh. Alternatively, if the installationâ€™s subs are overpowering, a user might dial the speaker trim up a bit (or effectively dial the subs down) to re-balance. In future versions, this compensation will be refined to be per-speaker or frequency-dependent using calibration data (e.g. a lookup table of correction gains for each speaker, measured by a mic). But initially, a single slider controlling all speakers is provided for simplicity. This gain is applied equally to all non-LFE channels, and it is not instantaneously applied (it is smoothed over ~50 ms to avoid any step changes). In implementation, we maintain a separate gain multiplier for the speaker bus that lerps toward the target compensation value. Because 50 ms is much longer than a block, this smoothing occurs over several blocks. We ensure that when the compensation is changed, we donâ€™t introduce a clickâ€”this is essentially another case of gain ramping.

Itâ€™s worth noting that speaker compensation could also address differences in distance or sensitivity of individual speakers (for instance, if some speakers are further from the typical audience area, one might boost them slightly). A manual per-channel calibration can be done by adjusting the content or layout, but the engine in future could incorporate a table of speaker gains. Our architecture has a logical place for that: after DBAP gains are computed, but before applying to audio, multiply by a per-speaker gain factor. For now, since exact calibration data isnâ€™t in scope, the single slider acts as a coarse control. Internally, it is implemented as just another gain multiplier in the audio threadâ€™s output loop for speakers.

Master Gain and Fade: Finally, the master gain is a global scalar on all output channels (speakers and subs). This is exposed as --master_gain (default 0.5 in offline renderer) to ensure the summed output doesnâ€™t clip the DACs. Summing many sources can raise overall levels, so setting master gain to 0.5 (âˆ’6 dB) is a safe default headroom. The master gain can be adjusted at runtime, and like other parameters, itâ€™s applied with smoothing. If a user quickly lowers the master volume, we apply a short exponential or linear ramp down to avoid a sudden drop that could cause a click (though volume drops usually click less than raises). Conversely, raising volume is ramped up over a few milliseconds. We treat master gain changes similarly to speaker compensation in terms of ramping.

When pausing or stopping playback, the engine can optionally perform a quick fade-out rather than a hard stop, to avoid truncating a waveform at non-zero crossing. For example, if the user hits pause, we might ramp master gain to 0 over ~20ms then stop, eliminating any transient. This behavior is typically subtle and can be handled outside the audio thread (e.g. by issuing a small fade command via the Control module).

All these gain adjustments (speaker trim, master gain, fade ramps) are linear multipliers applied to the final mix. We consolidate them mathematically so that the audio thread might compute a single multiplier per channel each frame (or even one per bus: one for all speakers, one for all subs). In practice, since subs arenâ€™t affected by speaker trim, weâ€™ll have two multipliers: e.g. for speaker channels: finalGain = masterGain \* speakerCompensation, and for sub channels: finalGain = masterGain (with their own internal calibration possibly). The audio thread multiplies each sample by the appropriate finalGain just before writing it to the output buffer provided by the audio API. This is done inside the callback, but itâ€™s O(n) over samples and channels which is negligible relative to the mixing work already done.

To summarize, the Speaker Compensation and Master Gain stage ensures the output can be tuned and controlled smoothly. It addresses both technical needs (avoiding clipping, matching levels across components) and perceptual tuning (slight balance adjustments). Importantly, all these operations are constant-time per sample and use pre-computed coefficients, so they are realtime-safe. The smoothing of any changes is done in a control context or via simple linear interpolation per sample (which is trivial DSP).

Output Remapping and Audio Backend

Responsibility: Map the engineâ€™s output channels to the physical audio device channels and interface with the chosen audio backend (AlloLib, PortAudio, JACK, etc.). Many multichannel audio systems have specific channel order requirements, so a flexible remapping is needed.

In our design, the output channel count is determined by the layoutâ€™s highest channel index (including subs). For example, if the highest-numbered speaker is 59 and subs are 60-61, then outChannels = 62. These logical channels (0â€“61) have specific meanings (0 might be speaker FL, etc., 60 might be sub1, etc.). The channel mapping configuration (see channelMapping.hpp) defines a one-to-one correspondence between these logical channels and the deviceâ€™s channels. In the AlloSphere case, for instance, some device outputs might not be used or are in a different order; the mapping file lists pairs like {fileChannel, deviceChannel} for each channel. In our engine, we will load a mapping (or use identity if none provided) at startup. The mapping is validated to ensure itâ€™s a true permutation or subset: each engine channel appears at most once as a source, and each device channel at most once as a target. Duplicating an output or merging channels is not allowed in v1 (if needed, that should be handled by sending identical audio to two engine channels explicitly).

During the audio callback, after the mix is prepared, we create the final output frame by frame. For each output frame (sample index) and each engine output channel, we find the mapped device channel and write the sample to that position in the driverâ€™s output buffer. If the mapping is identity (0->0, 1->1, ...), this is just a direct copy which we can optimize by memory copy. If not, we do a small index lookup. Given the mapping is static and small, this is negligible overhead. We can even unroll it if the mapping is known (like a static switch-case per channel).

For example, in the provided channelMapping.hpp (which was tailored to a 60-channel AlloSphere setup), some channels were skipped and one LFE was mapped to a particular output. Our engine would use a similar mapping: e.g., if channel 55 is LFE and should go to device output 47 (as in the example mapping), we ensure after mixing that engine channel 55â€™s samples are written into device channel 47. The mapping is applied to each sample, but since itâ€™s just indexing an array, itâ€™s effectively part of the final loop writing to the driver.

After remapping, the audio data is handed off to the audio backend API. The engine itself is agnostic to whether itâ€™s AlloLibâ€™s AudioIO, PortAudio, JACK, etc., as long as we can feed the interleaved (or non-interleaved, depending on API) samples. In AlloLib (which wraps portaudio or OS APIs under the hood), the callback gives an AudioIOData object with an output buffer we fill (as done in mainplayer.cpp). In PortAudio, our callback would be given pointers to an output array. In JACK, weâ€™d have one buffer per channel to fill. We handle these details in the Backend integration (next section and Appendix). The key point is that by the time we reach the output stage, we have a contiguous block of samples for each channel that just needs to be placed into the device buffers in the correct order.

We also include in this stage any final diagnostics: e.g., calculating levels for metering. In mainplayer.cpp, after writing outputs, they computed peak levels for each channel for VU meter display. We may include similar code if running a GUI, but importantly this must not interfere with realtime operation. In AlloLibâ€™s example, they did it inline in the callback (which is acceptable if itâ€™s just a few operations per sample). For more complex analysis, weâ€™d offload it. For now, we note that per-channel peak hold can be done with a few comparisons per sample â€“ this overhead for 60 channels \* block size 64 is minor.

Finally, the audio backend will deliver the audio to hardware with the configured buffer size and sample rate. The thread priority and device synchronization are managed by the backend (e.g., PortAudio will use callback at high priority, JACK runs its process cycle). As long as our callback returns within the allotted time per buffer, the audio plays glitch-free. Weâ€™ve designed all prior stages to ensure that.

In conclusion, the Output stage makes sure the right signals go to the right physical outputs and interfaces cleanly with whichever audio API is in use. The mapping system provides flexibility to adapt to various speaker wiring configurations without altering the spatialization logic. It enforces a one-to-one mapping (each logical channel has one destination) to avoid ambiguity. If an invalid mapping is loaded (e.g. two logical channels assigned to the same output), the engine will detect and reject that configuration at init â€“ preventing undefined behavior. In practice, we expect to use standard mappings (like those in channelMapping.hpp for AlloSphereâ€™s 54 speakers + subs). This separation of concerns (spatialization vs hardware layout) follows good design: the panner can work in a logical coordinate system, and the final mapping handles the quirks of hardware channel ordering.

Real-Time Threading and Safety Considerations

Achieving glitch-free audio under hard realtime constraints is a core requirement of this engine. This section outlines the thread model and the strategies used to guarantee timely audio delivery.

Audio Thread: The audio thread (sometimes called the callback thread or driver thread) is run by the audio backend at a high priority (real-time scheduling). It calls our processing callback at a regular interval (the buffer size divided by sample rate). For example, with a 64-sample buffer at 48 kHz, the callback is invoked every ~1.33ms. The audio thread must complete all mixing and output preparations before the next interval, or else an underrun (audible glitch) occurs. We therefore treat the audio thread code as an inner loop that must be as efficient and deterministic as possible. As emphasized in literature and docs, it cannot wait on locks, perform unpredictable operations, or allocate memory. Our design adheres to the following rules on the audio thread:

No mutex locks or blocking synchronization. All shared data structures between threads use lock-free techniques or are double-buffered. For instance, the control parameters are updated via atomics (no waiting), and the streaming buffers use atomics to flip states. By avoiding locks, we prevent priority inversion (where a high-priority audio thread could be blocked by a lower priority thread holding a mutex).

No dynamic memory allocation. All buffers, lists, and objects needed during playback are pre-allocated either at initialization or at worst during source activation (before starting the audio stream). The audio thread never calls new or malloc. Even resizing vectors in our code is done on the loader or setup thread (e.g. allocating a buffer to hold the next chunk). This ensures we donâ€™t hit a random page fault or heap lock in the middle of audio processing.

No file or network I/O. All disk reads happen on the loader thread, as described. The audio thread simply checks if data is ready (an atomic flag) and uses it. Similarly, any logging or network messages (e.g. OSC controls or debugging info) are done asynchronously. The audio thread might increment some atomic counters or set flags for the main thread to pick up and log outside of the callback.

Bounded computation: The algorithms used (gain calculations, loops over sources and speakers) are all bounded by fixed sizes (max 128 sources, 128 speakers for example). There are no unbounded loops dependent on input data or while() that could stall. This predictability is crucial. In practice, we estimate the worst-case processing and ensure it fits in the buffer time (see CPU safety below). Itâ€™s also recommended to avoid calling into OS or library functions that may not be real-time safe. For example, we wouldnâ€™t call a complex math library function that might internally allocate or use locks. Simple math (sqrt, sin, etc.) is fine as they donâ€™t allocate and typically run in constant time.

By following these guidelines (which echo known rules from PortAudio and expert articles), we ensure the audio thread can consistently meet its deadlines. In case of extreme CPU load where itâ€™s just barely missing deadlines, we prefer to degrade audio quality rather than violate these rules (discussed in CPU Safety Manager). Under no circumstance will we â€œfixâ€ a realtime glitch by, say, adding a sleep or lock â€“ that would only worsen the situation.

Loader/Control Threads: We have at least one loader thread (for file I/O) and possibly the main thread or GUI thread feeding controls. These threads run at normal priority. They can afford to use locks (like the file read mutex) or allocate memory (loading new files, etc.) because any hiccup there will not directly glitch audio â€“ it might cause an indirect effect like a buffer not being ready (which we handle via fallback). The communication from these threads to the audio thread is carefully designed via non-blocking means. For example, when the loader has filled a buffer, it sets an atomic flag to READY and possibly uses a lock-free queue to notify the audio thread (or the audio thread just checks flags each cycle). The main control thread, when updating focus or other params, writes to an atomic variable that the audio thread reads. We essentially implement a lock-free message passing system for audio updates. This is in line with professional game audio engines: â€œThe game thread communicates audio commands through lock-free data structures like ring buffers, avoiding locks and memory allocation in the audio thread.â€.

Buffer Size and Latency: The frame buffer size (sometimes called framesPerBuffer) is a critical knob. Smaller buffers -> lower latency but more frequent callbacks (higher CPU overhead and risk of underrun). Larger buffers -> higher latency but more slack for processing. Our engine is intended for interactive audio in an immersive space, so we value reasonably low latency (so that moving a sound or reacting to user input is timely). A buffer size of 128 at 48kHz is ~2.7ms, 256 is ~5.3ms, etc. In many pro audio settings, 128 or 256 is a common choice. We will choose a default (perhaps 256 frames) and allow tuning. The documentation will recommend ranges (e.g. 128â€“512) and note trade-offs. If using JACK, the buffer size might be configured externally by the JACK server. If using PortAudio/CoreAudio, we can request 128 or 256. Notably, some host APIs yield better stability with certain buffer strategies: PortAudio notes that some OS backends can glitch with small fixed buffers under heavy CPU and suggests using the special paFramesPerBufferUnspecified to let the host choose optimal scheduling for heavy load. We will test on our target systems and advise accordingly. The engineâ€™s internal logic does not depend on a particular buffer size except that very large buffers would reduce the control update rate granularity (but 512 or 1024 is still fine for control changes usually).

CPU Safety Manager: Despite careful optimization, there is always a chance the audio thread might approach its time budget when many sources are active. Our design includes a CPU safety mechanism (outlined in realtime_context_notes.md) to degrade processing gracefully. The idea is to monitor the audio callback duration (perhaps via Pa_GetStreamCpuLoad() or our own high-res timer around the callback) and compare it to the buffer duration. If we consistently use, say, >90% of the available time, itâ€™s a warning. If we hit 100% (an underrun likely), itâ€™s critical. To prevent sustained overload, the engine can step down through quality levels:

Level 0 (Full Quality): Recompute gains every block (highest temporal precision). This is the normal mode.

Level 1: Recompute gains every 2 blocks, i.e., only update spatialization 25% as often, effectively holding the same gains for 2 consecutive blocks. This halves the computation of the panning matrix at the cost of slightly laggy or stepped motion. If sources are moving slowly, itâ€™s nearly inaudible; if moving fast, a slight stutter in motion might be heard but better than a buffer drop.

Level 2: Recompute gains every 4 blocks. Further reduction in CPU use, more potential spatial error for fast-moving sources.

Optional Level 3: If enabled, use a Top-K optimization â€“ for each source, only mix the K loudest speakers (e.g. K=3 or 4) and ignore the rest. This drastically cuts multiplication count at the expense of spatial fidelity (the sound might lose some coverage or power consistency). This is only to be used as a last resort (and maybe user-toggleable, not automatic, since it changes the spatial rendering noticeably).

The CPU manager can automatically toggle between these levels based on the measured load, with hysteresis to avoid rapid toggling. For example, if CPU > 90% for 3 consecutive callbacks, drop one level; only return to higher quality after CPU < 70% for some time, etc. The thresholds and hysteresis will be tuned (and can be referenced from audio engine best practices or our tests). This approach ensures that in extreme scenes (say all 128 sources active and moving), the system will not simply start glitching but will instead sacrifice some spatial accuracy to stay within time. Since our primary goal is uninterrupted audio, this is a valuable trade-off. And because itâ€™s reversible (if load reduces, we can ramp quality back up), the effect on the user experience is minimized.

Crucially, the CPU safety changes themselves must be implemented in a realtime-safe way. They typically just involve skipping certain computations on some callbacks â€“ which is just a if(frameCount % N == 0) compute gains kind of logic, which is fine. Adjusting K for Top-K might be more complex (needing sorting gains). If we do implement Top-K, we would likely compute the sorted speaker gains in the background thread based on the last known positions, and just instruct the audio thread which speakers to include (to avoid sorting in the audio thread). However, Top-K is a future/optional feature; initial implementation will focus on block-skipping which is trivial to do in the callback.

Testing and Worst-case: We will heavily test the engine under worst-case scenarios (128 sources all active, all moving rapidly, focus at default or low (worst case because wide focus uses all speakers)). We will measure the callback times with different buffer sizes. Modern CPUs can typically handle thousands of operations in a sub-millisecond window, but algorithmic efficiency and memory access patterns matter. Weâ€™ll profile and ensure no unexpected slow paths. Also, enabling compiler optimizations (like -O3 and using appropriate data structures) will be important.

In summary, the engineâ€™s threading model cleanly separates duties: a realtime audio thread that does deterministic mixing, and auxiliary threads for anything slow. By following established rules and leveraging lock-free communication, we prevent common audio bugs (clicks, dropouts). The CPU safety system adds a layer of resilience by proactively scaling back work if needed. Combined with careful buffer size selection and potential use of high-performance backends (JACK, ASIO, etc.), the engine will meet hard realtime criteria.

Appendix: Audio Backend Options (AlloLib vs PortAudio vs JACK)

The design above is backend-agnostic in core logic, but choosing an appropriate audio I/O backend is important for integration, performance, and portability. Here we compare three options:

AlloLib AudioIO (current prototype): AlloLib is a multimedia framework developed by the AlloSphere Research Group, which includes an AudioIO module for sound output. The prototype player (mainplayer.cpp) already uses AlloLib: calling app.configureAudio(sampleRate, bufferSize, outChannels, 0) sets up the audio device, and AlloLib handles the rest. AlloLib internally uses a callback mechanism similar to PortAudio. In fact, AlloLibâ€™s AudioIO is built on top of either the operating systemâ€™s API or on PortAudio/RtAudio â€“ it provides a convenient C++ interface with integration into AlloLibâ€™s App framework (which ties together audio, graphics, and UI).

Pros: Using AlloLib means minimal code changes for us â€“ our engine can be integrated as an AlloLib AudioCallback (as done in adm_player.onSound()). It supports the needed channel counts (demonstrated with 60-channel output in the AlloSphere case) and ties into AlloLibâ€™s threading (so our audio thread priority is handled). It also allows easy use of AlloLibâ€™s GUI for debugging (e.g. ImGui UI in the same app).

Cons: AlloLib is less widely used outside the lab. If we want our engine to run standalone or on other systems without the whole AlloLib, we might prefer a more common library. Also, AlloLibâ€™s audio might rely on PortAudio internally in some builds, which could introduce the same issues noted below for PortAudio on certain platforms. AlloLib is great for the AlloSphere environment and rapid development, but for distribution or integration into other projects, a standard backend might be easier.

PortAudio: PortAudio is a cross-platform audio I/O library that provides a unified API to CoreAudio (Mac), ASIO/WASAPI (Windows), ALSA/JACK (Linux), etc. It uses a callback style which fits our engine well. We could register our audio callback with PortAudio and get a high-priority thread calling it. PortAudio is widely used and would allow our engine to run on Windows (which AlloLib might not support out of the box for multichannel ASIO, for example).

Pros: PortAudio supports many platforms and device types. It can handle multi-channel devices â€“ we can open a stream with, say, 62 output channels, if the device/driver supports that. Itâ€™s open-source and widely tested. By using PortAudio, we align with common practice for standalone audio applications. Additionally, PortAudio has a built-in measurement of CPU load and can invoke our callback at appropriate priority.

Cons: PortAudio has some known quirks, especially with high channel counts and Windows drivers. A developer experience report noted that on Windows, the only reliably working backend was the older MME (winmm) which has high latency, and that enabling ASIO or WASAPI required custom fiddling. It was also mentioned that PortAudio doesnâ€™t automatically handle channel order differences â€“ the developer had to manage channel mapping manually for surround beyond stereo. For our use-case, we already have a mapping layer, so thatâ€™s fine. But the latency issue is a consideration: PortAudio can use ASIO for low latency, but depending on configuration it might not choose it by default. We might have to explicitly choose host API (PortAudio lets you enumerate and pick ASIO on Windows). Another consideration: PortAudio plus our high thread count might need careful buffer size tuning to avoid glitching, as itâ€™s a general library not specifically optimized for the lowest possible latency on all systems (JACK, by contrast, is laser-focused on low latency).

In summary, PortAudio gives portability but might need some extra care for performance. Itâ€™s a solid default choice if we aim to distribute the engine widely.

JACK: JACK (Jack Audio Connection Kit) is a professional, low-latency audio server primarily on Linux (also available on macOS and Windows with some setup). JACK would run as a separate process (audio server) and our engine would be a JACK client. JACK offers extremely reliable low-latency performance and the unique ability to route audio between applications. For example, our engineâ€™s 60 outputs could be connected to another application or measurement tool in software. JACKâ€™s callback API is quite similar to PortAudioâ€™s, making porting straightforward. In fact, PortAudio has a JACK backend, and JACK can be one of PortAudioâ€™s host APIs, meaning if our engine is written for PortAudio, it could potentially run through JACK by selecting that host.

Pros: Unmatched low-latency performance on Linux; designed for real-time from the ground up. If we need to integrate with other audio software (say spatial audio frameworks or effects processors) JACK is ideal since it allows inter-application connections. JACK also ensures consistent timing: if multiple apps are running, they all process in lockstep, which could be beneficial for syncing with a visual system perhaps. Another pro is that JACK provides xrun (underrun) detection and measurement tools, helping us debug timing issues.

Cons: Added complexity for users not already using JACK. On Linux pro audio systems, JACK is common, but on Windows, setting up JACK is non-trivial (and ASIO is more common). On macOS, CoreAudio already provides low latency, so JACK is less used (except by some advanced users). Essentially, requiring JACK would limit our user base to those comfortable with it. Another con is that using JACK ties us to running as a client in a larger system, which might not be necessary if we just want to output to a device directly. If our application is self-contained (like a dedicated AlloSphere player), running JACK might be an unnecessary layer unless we explicitly want to route audio to different endpoints.

Recommendation: For development within the AlloSphere context, AlloLib is convenient and already integrated â€“ we will continue to use it for now (it likely uses PortAudio or RtAudio under the hood, but we can rely on its abstraction). For broader deployment or if we make this engine a standalone library, supporting PortAudio is wise for cross-platform support. We can abstract our audio device interface so that either AlloLibâ€™s AudioIO or PortAudio can be used (perhaps via compile-time switch or a small adapter class that calls into whichever). If ultra-low latency and multi-application routing is needed, supporting JACK natively is a good option, especially on Linux. JACK and PortAudio are not mutually exclusive: as noted, a PortAudio app can even use JACK as a backend. We might also consider RtAudio (another cross-platform API similar to PortAudio) which is a single-header library supporting ASIO, CoreAudio, ALSA, JACK etc., but since AlloLib and our team already have experience with PortAudio, weâ€™ll stick to that.

From a performance perspective, using the CoreAudio API directly on macOS might yield slightly lower overhead than PortAudio, but the difference is small. CoreAudio is conceptually similar to JACKâ€™s callback approach (but without inter-app routing) and PortAudio essentially wraps it.

One real-world note: if using PortAudio on Windows for >2 channels, we should use the ASIO backend (since the default WDM/DS may expose only stereo pairs and have high latency). ASIO is proprietary but many pro interfaces have drivers. PortAudio can use ASIO if available. The Horror story in the blog suggests PortAudioâ€™s other backends were problematic and that direct use of WinMM was too latent. So, if targeting Windows, weâ€™ll ensure to test multi-channel output and possibly instruct the user to install ASIO drivers for their sound device (or we bundle an ASIO solution like ASIO4All for generic soundcards, though thatâ€™s not ideal for 60 channels â€“ usually a dedicated interface is needed).

Summary: AlloLib is great for integration with our existing code and has been proven with our 54-channel output. PortAudio offers cross-platform reach and will be our likely choice for a standalone engine. JACK is best for specialized scenarios requiring inter-app audio or ultra-low-latency on Linux. We can design our engine to be flexible: abstract the audio output so that switching backends is easy (for instance, provide an interface where the engine produces blocks of audio that are then given to whatever backend client).

(Footnote: The CSL framework from MAT mentioned supporting PortAudio, RtAudio, JACK, etc., showing that many frameworks end up abstracting multiple backends. We may not need such complexity initially, but itâ€™s good to keep in mind.)

Appendix: Future Extensions and Features

Looking beyond the initial implementation (v1), several enhancements and features are planned or under consideration:

Per-Speaker Calibration (LUT Compensation): In v1, we use a simple manual speaker trim slider. Future versions will include a lookup table (LUT) of gain adjustments for each speaker to compensate for physical variations (distance, speaker sensitivity, room acoustics). The procedure would involve measuring the output of each speaker (with test signals and a reference mic) and computing gain offsets so that a sound panned to each speaker yields equal perceived level at the center. These offsets (likely in dB) would then be loaded into the engine and applied as a per-speaker gain multiplier in the spatialization stage. This will correct for â€œhotâ€ or â€œcoldâ€ speakers and for the focusing effect of DBAP at very close distances. Implementing this will require minimal realtime overhead (just a multiply per speaker), and the calibration could be done offline. We envision providing a small calibration utility and then feeding its results (the LUT) into the engine at startup.

Solo and Mute Controls per Source: Debugging and mixing workflow can benefit from ability to solo a single source (or a subset) and mute others. While not in the core spec for v1, we have partially planned for it (phase 2 mention in notes). Solo-ing a source means that in the spatial mix, only that source is output on the speaker channels, while all others are suppressed. However, any LFE channels should remain unaffected (if soloing one object, we generally still want the full LFE track playing if itâ€™s part of the content, unless we specifically solo an LFE). The implementation would involve a quick mixing rule: if soloSourceId is set, then when summing sources, skip all sources except that ID (for speakers). Alternatively, scale non-solo sources by 0. Itâ€™s straightforward in code. We just need to ensure the transition into/out of solo is smooth (likely ramp down others over a few ms). Solo is mainly for monitoring or debugging in an installation scenario (to hear an individual objectâ€™s contribution). Similarly, a Mute flag per source (or a list of muted sources) can be honored by simply not mixing those sources (similar to being inactive).

Seek/Scrub support: Currently, the engine is focused on continuous playback. If we want to support jumping to an arbitrary time (scrubbing the timeline), we need to flush and reset the streaming buffers to the new position. In practice, this means adding an API like seekTo(timeSec). The loader thread would then seek all file handles to the corresponding frame (using soundFile.seek() for each source) and refill buffers from that point. The frameCounter would be reset to the sample index of that time. Weâ€™d also update any state in the Pose module (advance or rewind animations to that time, etc.). The challenge is doing this without glitching: weâ€™d likely pause audio output for a brief moment (one could crossfade from old to new position audio, but that doesnâ€™t musically make sense unless specifically desired). A simpler approach is to stop the stream (or flush the output by filling with silence) for a cycle or two while new data buffers are prepared, then resume at the new position. Given this is a deliberate user action (not continuous), a tiny gap is acceptable if it ensures alignment. However, we can minimize gap by preloading the new positionâ€™s chunk in advance if user scrub is predictable (like dragging a seek bar). Implementation aside, adding seek capability would greatly help in rehearsing sections of content or implementing looped playback from any point.

Additional Spatialization Modes: The engine architecture is built around DBAP for v1, but it is designed to be extensible. We could integrate VBAP (Vector Base Amplitude Panning) and Ambisonics as alternative spatializers. In fact, the offline renderer already has VBAP and LBAP modes. In realtime, VBAP would require computing a different gain set (only 3 speakers active per source typically), which could be more CPU-efficient for many sources. We would need to precompute the VBAP triangulation of the speaker layout (which is doable at init). Ambisonics would involve encoding sources into spherical harmonics and decoding to speakers â€“ more CPU but very scalable for many sources due to linearity. These are longer-term possibilities; the engineâ€™s modular structure (source positions in, gains out) means we can swap the algorithm inside the Spatialization mixer. Weâ€™d want to expose a --spatializer option to choose (just as the offline CLI does). For now, DBAP is default and robust, but VBAP could be offered for situations where a sweet spot is acceptable and CPU needs to be lower (because VBAP engages fewer speakers per source, it reduces per-sample mixing cost significantly). The Appendix from SPAT Revolution docs confirms DBAP and others can coexist in a system; our engine might allow switching on the fly or per scene basis.

3D Elevation Modes & â€œAtmosâ€ compatibility: As noted, we have modes like RescaleFullSphere vs RescaleAtmosUp implemented (currently in offline). We will ensure those are in realtime as well. This effectively is already handled in Pose module. Future refinement might include dynamic mode switching based on listener position or other criteria (though DBAP doesnâ€™t need a listener position, sometimes for sound design one might simulate a virtual â€œear level compressionâ€). We could also implement distance delay (like adding propagation delay for far sources) and Doppler effects â€“ however, these veer into more advanced spatialization (and DBAP itself doesnâ€™t inherently cover time-of-arrival). Possibly not needed in AlloSphere where physical speaker distances are small relative to speed of sound differences (and it might complicate syncing with visuals).

Priority-Based Voice Management: Similar to game audio systems, if we ever have more sources than we can handle, we could implement a priority system to drop or virtualize some. For example, if we had 200 potential sources but only 128 can be mixed, weâ€™d sort by importance (which could be based on loudness, distance, or a priority value) and only mix the top 128 at any given time. Lower ones could be either not played or â€œvirtualizedâ€ (i.e., kept alive but not output, so if they become important later, we can resume without losing sync). This is a complex feature and likely beyond the scope of typical use (128 is already quite high), but itâ€™s a concept aligned with the CPU safety as well â€“ instead of degrading all voicesâ€™ quality, one might drop the least important voices entirely to save CPU. Implementing this would require sorting per block or every few blocks which might be heavy, so itâ€™s something to carefully design (maybe combine with Top-K or as a separate mode).

Networking and Distributed Audio: The AlloSphere sometimes uses distributed rendering (multiple machines each handling a subset of speakers). Our design currently assumes one machine outputting to the whole array. In future, we might extend it to support a distributed mode where sources are spatialized across nodes. For example, divide the speaker layout into clusters per machine and network the audio or data. That would involve sending either audio stems or position data over network to other instances of the engine. While intriguing, this veers more into system integration than core engine, so it might be tackled later as needed by specific installations.

Integration with ADM/BWF pipelines: Upstream, the engine relies on the LUSID scene file (which itself is derived from an ADM file in our workflow). Future improvements might include tighter integration such that the engine can directly ingest ADM metadata and audio tracks without a separate conversion step. For instance, an embedded ADM parser (like EBU libadm as is being integrated in AGENTS.md context) could feed our engine. This doesnâ€™t affect the realtime audio part much, except maybe at initialization (parsing an XML etc.). It could also allow dynamic reconfiguration if new scenes are loaded on the fly.

UI and Monitoring: We may build a simple UI overlay (using e.g. ImGui) to visualize the current CPU load, active voices, etc., in realtime. The engine will maintain counters such as current DSP load % (from PortAudio or manual measurement), and number of sources active, which can be displayed. This helps during rehearsals to identify if weâ€™re near limits and how the safety features are behaving.

Each of these future features will be carefully evaluated for realtime safety and added in a modular fashion. The current design lays a strong foundation â€“ thread-safe, efficient, and with hooks (like the elevation mode, focus, compensation, etc.) that can be extended without overhauling the structure. The overarching goal remains to provide a stable spatial audio engine that can evolve with new algorithms and adapt to different production needs, while always maintaining the hard realtime performance that ensures the audience hears a continuous, immersive soundfield with no interruptions.

References:

Lossius, T., Baltazar, P., & de la Hogue, T. (2009). Distance-Based Amplitude Panning (DBAP). (Describes the DBAP algorithm and its assumptions, forming the basis of our spatializer.)

PortAudio Documentation â€“ Realtime Programming Guidelines. (Emphasizes avoiding memory allocation and I/O in audio callbacks.)

Tyson, M. â€œFour Common Mistakes in Audio Development.â€ atastypixel.com. (Highlights the importance of not blocking the audio thread and maintaining glitch-free performance.)

AlloSphere Research Group â€“ AlloLib and Audio framework (2026). (AlloLibâ€™s AudioIO provides the multi-channel audio callback system used in our prototype, see mainplayer.cpp in sonoPleth for usage.)

Generalist Programmer. "Game Audio Programming: Complete Sound Engine Guide 2025." (Discusses voice management and lock-free communication, principles we applied in our CPU safety and threading model.)

JACK Audio Connection Kit FAQ. (Compares JACK and PortAudio, noting PortAudio focuses on hardware I/O and JACK on inter-app routing, informing our backend considerations.)

Libaudioverse Development Blog (2014). (Reports practical issues with PortAudio on Windows for multi-channel audio, guiding our approach to backend selection and testing.)
