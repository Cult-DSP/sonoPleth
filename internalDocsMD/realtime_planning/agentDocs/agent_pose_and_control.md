# Pose and Control Agent

> **Implementation Status: ✅ COMPLETE (Phase 3, Feb 24 2026)**
> Implemented in `spatial_engine/realtimeEngine/src/Pose.hpp`.
> See `realtime_master.md` Phase 3 Completion Log for details.
> Key design: SLERP keyframe interpolation, elevation sanitization
> (Clamp / RescaleAtmosUp / RescaleFullSphere), DBAP coordinate transform.
> `computePositions()` is audio-thread-only; all data read-only after `loadScene()`.
> Thread ownership contracts documented in Phase 8 (see `Pose.hpp` header).

## Overview

The **Pose and Control Agent** manages dynamic scene parameters, including the positions and orientations (poses) of sound sources and the listener, as well as handling external control commands. In a spatial audio context, “pose” typically refers to spatial coordinates and rotation of an object (here, sound sources and possibly the listener or microphones). This agent ensures that the Spatializer always has up-to-date spatial metadata for every sound, which is crucial for accurate rendering via the DBAP algorithm. It also processes control inputs from the user or other systems – for example, commands to add/remove a source, change a source’s properties, or global engine settings.

Running on a separate thread (or integrated with the main thread if it’s event-driven), the Pose and Control Agent isolates non-audio real-time tasks such as responding to UI input, network messages (e.g., OSC commands for spatial positions), or scripted events. It maintains a shared data structure representing the current “scene” (all active audio objects and their spatial attributes), which it updates safely without disrupting the audio thread.

## Responsibilities

- **Position and Orientation Updates:** Track the 3D position (and potentially orientation) of each audio source and the listener in real time. For example, if a source is moving (due to an in-game event or user input), this agent receives that information and updates the source’s coordinates. Similarly, if the listener’s position (reference point for spatialization) changes, update it promptly.
- **Control Commands Handling:** Interpret and execute various control instructions:
  - Adding or removing sound sources (e.g., create a new source in the scene with given audio stream and initial position).
  - Changing source parameters (e.g., adjusting a source’s gain, muting/unmuting, or switching a source on/off).
  - Global controls like pausing the entire engine, changing the overall spatialization mode, etc.
- **Scene State Management:** Maintain a central data model of the audio scene (sometimes called a “scene graph” or object list). This includes each source’s state (position, orientation, velocity if needed, volume, etc.) and possibly environment parameters. The agent ensures this state is consistent and can be queried by other parts of the system.
- **Interpolation and Smoothing:** If position updates come at a lower rate or irregular intervals (for instance, from a tracking system or user dragging an icon), the agent may interpolate or smooth these updates so that the Spatializer sees a smoothly changing position every audio frame. This prevents audible artifacts from sudden jumps in position. It might not do heavy DSP itself, but could, for example, maintain velocity and compute estimated positions in between updates if needed.
- **Synchronization of Pose with Audio:** Ensure that updates to positions happen in sync with audio frames to avoid phase inconsistencies. This often means applying new positions at frame boundaries. The agent might timestamp updates or sequence them such that the audio thread uses a coherent set of positions for each block of samples.

## Relevant Internal Files

- **`SceneState.h` / `SceneState.cpp`:** A module that defines the data structures for the audio scene (e.g., a struct or class containing lists of sources and a listener). The Pose and Control Agent will be the primary writer to this structure, while the Spatializer (and possibly GUI) are readers.
- **`mainplayer.cpp`:** The main orchestrator might call into this agent to initialize the scene or in response to user input. For example, mainplayer could catch a UI event (like “add source”) and delegate to Pose and Control to execute it. It might also create the Pose/Control thread on startup.
- **`ControlInput.cpp`:** If there’s a system for parsing external commands (keyboard, MIDI, OSC network messages, etc.), that logic would reside here. The Pose and Control Agent would use it to translate those inputs into engine actions (like moving a source).
- **`SpatialRenderer.cpp`:** While mostly part of the Spatializer, this file (or related ones) will read data that Pose and Control produces. There might be code in SpatialRenderer that calls something like `sceneState.getSourcePosition(id)` each frame. The interface between Spatializer and Pose/Control is defined through these data accessors.

## Hard Real-Time Constraints

Most of the Pose and Control Agent’s work is on the **control thread**, which is not in the hard real-time audio path. However, its operations still influence the real-time thread and thus must be designed carefully:

- **Non-Blocking Data Publishing:** When this agent updates the shared scene state, it must do so in a way that never causes the audio thread to wait. A common strategy is double-buffering or atomic swaps. For example, keep two copies of the scene data – one that the audio thread is currently reading, and one for updates. Once updates are applied, swap pointers atomically at a safe time (e.g., audio frame boundary) so the audio thread sees the new data. This avoids the need for locks in the audio callback.
- **Timely Updates:** Spatializer will typically use the latest positions for each audio frame. The Pose and Control Agent should aim to provide fresh data as soon as possible after it’s received. However, if updates come very rapidly (e.g., tracking at 120 Hz) while audio frames are at 48 kHz (≈ 1000 frames per second for 512-sample blocks), the agent might drop some intermediate updates or always use the most recent one to avoid backlog. The key is that the position used during an audio frame is as up-to-date as possible, but not changing mid-frame.
- **Thread Priority:** The control thread can run at a normal priority (lower than the audio thread). If it falls slightly behind (e.g., heavy computations, which should be rare), it’s better than the audio thread falling behind. But ensure it’s not starved completely; it must still update the scene at a reasonable rate. Use OS scheduling hints if needed to keep it responsive (but still below audio thread).
- **Avoid Heavy Computation in Control Path:** If a control command requires heavy computation (for instance, loading a huge configuration or path-finding for moving objects), consider offloading that or doing it gradually. The control thread should ideally not stall for long periods because it might hold resources (like a mutex to update scene state) briefly. Keep typical position updates light (just copying a few floats).
- **Memory Management:** Similar to audio thread, avoid frequent allocations for continuous updates. Data structures for sources should be pre-allocated or use object pools for new sources. This prevents sporadic stalls that could coincidentally block an audio read if not careful. While not as critical as the audio thread, a long GC or allocation pause here could delay positional updates.

## Interaction with Other Agents

- **Spatializer (DBAP) Agent:** This is the primary consumer of the data managed by Pose and Control. The Spatializer will query source positions each audio frame. The interaction must be carefully designed: e.g., Spatializer might hold a pointer/reference to a read-only snapshot of positions that Pose/Control updates each block. The two agents should agree on data formats (coordinate system, units) and any conventions (like what happens if a source is not updated – assume last position).
- **Streaming Agent:** When a new source is added or an existing one removed (via a control command), the Pose and Control agent will likely coordinate with the Streaming Agent. For example, if a user requests a new sound at a certain position, Pose and Control may create a source entry and then trigger the Streaming Agent to start loading the audio for that source. Conversely, if a source is stopped, Pose and Control might mark it inactive and instruct Streaming to stop feeding it.
- **Compensation and Gain Agent:** Some control commands might relate to gains or attenuation models (e.g., turning distance attenuation on/off for a source, or adjusting a source’s manual gain). In such cases, Pose and Control would forward those settings to the Compensation and Gain Agent or directly update parameters that compensation logic uses. For example, if a user sets a source’s gain via the UI, Pose and Control might set a field in the source state which the Compensation agent then uses in the audio thread.
- **GUI Agent:** The GUI is a major source of events – user-driven changes to the scene. The Pose and Control agent acts on these events. For instance, if the user drags a sound icon to a new location on a map, the GUI agent will send the new coordinates to the Pose and Control agent, which then updates the scene state. Conversely, Pose and Control might notify the GUI of changes initiated elsewhere (like if a source automatically moves via a script, the GUI may need to update its display). They will share data (possibly through the same scene state structure or via observer callbacks) in a thread-safe manner.
- **Threading and Safety Agent:** Implementation of Pose and Control will utilize the threading guidelines for safe data sharing. If a lock or mutex is used for the scene data, it must be carefully managed (e.g., only locked on the control thread side, never on audio thread). Alternatively, the lock-free design (double-buffer) would be advised by the Threading agent. The Pose and Control agent developer should coordinate on these details to ensure consistency across the engine.
- **Backend Adapter Agent:** Generally, not much direct interaction, since backend deals with audio output. However, if a control command involves changing output device or format (which is more of a backend issue), the Pose/Control might pass that request to the Backend Adapter or global engine level. For example, switching between headphone (binaural) and speaker output might be initiated by a control command that leads to reinitializing the backend.

## Data Structures & Interfaces

- **Scene Graph / Source List:** A central data structure, e.g., `struct AudioSourceState { id, position (x,y,z), orientation (qx,qy,qz,qw), gain, isActive, etc. }`. All sources could be stored in a vector or map keyed by an ID. The Pose and Control agent will add/remove or update entries here. The Spatializer will read from this structure for each source’s position and gain. If using double-buffering, there might be two copies: `sceneStateA` and `sceneStateB`, and an atomic pointer indicating which is current.
- **Listener State:** A separate struct for the listener’s pose (or listening point). DBAP doesn’t strictly require the listener position if it assumes relative positions are all that matter (since it’s distance-based panning independent of listener within the speaker array). However, if we consider rendering relative to a listener (e.g., the listener can move in the space, affecting apparent source distances), we include a listener position. Pose and Control would update that if, say, the user’s head moves in VR or the camera moves in a game.
- **Control Command Queue:** The agent might implement a thread-safe queue for inbound commands (especially if commands come from multiple sources like GUI thread or network thread). For example, an `std::queue<ControlCommand>` protected by a mutex or an atomic ring buffer. The Pose and Control thread would pop commands and execute them (update state, call other agents as needed). This decouples immediate UI events from actual state mutation, smoothing out bursts of commands.
- **Interfaces/Methods:** Provide methods like `updateSourcePosition(id, newPosition)` or `addSource(id, params)` and `removeSource(id)`. These would be called by external entities (GUI or main thread). Internally, these might push a command to the queue if needed, or lock and update state directly if safe. Additionally, an interface for Spatializer like `getSceneSnapshot()` that returns a pointer or reference to the current read-only scene data that the audio thread can use per frame.
- **Timestamping:** Each update could carry a timestamp or frame number (especially if doing interpolation). If needed, the agent might calculate intermediate positions for the exact audio time if positions are coming at a lower rate. This might not be fully implemented initially, but the design can allow for it (e.g., storing last update time and current, so Spatializer could optionally interpolate between them if it has time information).

## Development and Documentation Notes

For the engineer working on the Pose and Control Agent:

- **Clarify Interfaces:** Early on, decide how the Spatializer will access the data. Document in this file how the data handoff works (e.g., “Spatializer calls `SceneState::lockForRead()`” vs “Spatializer reads an atomic pointer to current scene”). Make sure this matches what is written in the Spatializer’s documentation.
- **Update Master Doc:** If any assumptions here differ from the overview (realtime_master.md), update that file. For example, if initially we planned double-buffering but then choose a lock-free single structure with atomics, reflect that in the master design as well so all team members know the approach.
- **Coordination:** Communicate with the developers of Streaming, GUI, and others. Document any agreed-upon command formats or data formats in `internalDocsMD/agents.md` for consistency. For instance, if positions are in meters in a right-handed coordinate system with Y-up, state that clearly here and ensure GUI uses the same.
- **Progress Reporting:** Note major milestones, such as “Basic add/remove source implemented” or “Connected OSC control for source movement” in the agents index (`internalDocsMD/agents.md`). This helps track how far the integration has come.
- **RENDERING.md Updates:** If the rendering pipeline description in RENDERING.md covers how spatial positions affect audio, ensure any change in logic (like adding interpolation, or deciding to include/exclude listener position in calculations) is updated there. That document might have a section on “Spatial Rendering Inputs” which should align with what Pose and Control provides.
- **Testing Plan:** Plan how to test this component (e.g., moving a sound in a circle to see if audio pans correctly). Document any test procedures or scripts in this file or a linked testing doc. Hard real-time doesn’t directly apply here, but ensuring that updates at 60 Hz do not break audio at 512-sample frames is something to validate.
- **Future Handoff:** Write down any pending concerns or future enhancements (for example, “Integration with tracking system X” or “Support Doppler effect if needed later”). This will help future developers or when revisiting the agent for upgrades.

By thoroughly defining the responsibilities and interfaces, the Pose and Control Agent will enable dynamic, interactive spatial audio scenes, keeping the audio engine informed of “who is where” at all times without compromising real-time performance.
