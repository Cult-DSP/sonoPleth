# Spatializer (DBAP) Agent

## Overview

The **Spatializer (DBAP) Agent** is the core audio processing component of the engine, responsible for rendering multiple sound sources into a spatial audio scene using the Distance-Based Amplitude Panning (DBAP) algorithm. It takes audio frames from each source and computes how loud each source should be in each output speaker channel based on the distances between the source and the speakers. The result is a multi-channel audio mix that gives the illusion of sounds originating from specific points in space.

DBAP is a flexible panning technique that does **not** assume the listener is in a sweet spot; instead, it uses the actual distances to each speaker to distribute sound intensity:contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}. In this engine, the Spatializer uses DBAP to accommodate arbitrary speaker layouts, meaning it can support standard setups (like stereo or 5.1) as well as custom speaker arrangements. The agent’s work happens in the real-time audio thread (often as the audio callback) and thus is highly optimized and constrained by hard real-time requirements.

## Responsibilities

- **Gain Calculation per Source per Speaker:** For each active sound source, calculate a gain factor for each output channel (speaker) based on the source’s position relative to that speaker. In DBAP, closer speakers get higher gain, and far speakers get lower gain, often with a normalization so that the total power remains constant:contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5}. The agent implements this formula. It may update these gain coefficients whenever a source or listener moves significantly (not necessarily every sample if positions are static over a block).
- **Audio Mixing:** Apply the calculated gains to each source’s audio frame and sum contributions into each speaker channel buffer. Essentially, the Spatializer forms a weighted sum of all sources for each output. If there are N sources and M speakers, it performs N×M multiplications and accumulations per audio frame (minus any optimization for inactive sources or silent frames).
- **Distance Attenuation (Coordination):** Ensure that as a source gets farther away, its overall volume diminishes appropriately. DBAP by itself handles the distribution but not the absolute level drop with distance (unless integrated with an attenuation factor). The Spatializer should either integrate a distance attenuation formula (e.g., inverse-square law or a custom rolloff curve) into the gain computation or work with the Compensation agent to do so. This prevents all sources from sounding equally loud regardless of distance.
- **Maintain Audio Fidelity:** Perform the above operations without introducing artifacts. This includes avoiding sudden changes in gain that could click (applying smoothing to gain changes if a source moves quickly or appears/disappears), and handling edge cases like a source being co-located with a speaker (which could lead to extremely high gain on one speaker unless capped).
- **CPU Optimization:** Use efficient math and data structures to minimize CPU use. For example, precompute any static factors (like if a speaker layout is fixed, possibly compute an invariant portion of the gain formula for each speaker position). Leverage vectorization (SIMD) for applying gains to audio arrays if possible. If the number of sources or speakers is large, consider spatial culling (don’t process sources that are effectively silent at a given listener location, though with DBAP typically all speakers contribute, but perhaps some thresholding can be applied).

## Relevant Internal Files

- **`SpatialRenderer.cpp`:** The core implementation likely resides here. This file contains the real-time audio callback or processing loop that calls the spatialization mix. It will have functions like `renderAudioFrame()` that iterate over sources and speakers to perform DBAP mixing. The Spatializer agent’s logic will be implemented in this module.
- **`SpatializerDBAP.h` / `.cpp`:** If the design separates the algorithm into a separate class, this would encapsulate the DBAP algorithm (distance calculations and gain matrix computation). It might include configuration like the rolloff exponent or normalization method.
- **`SpeakerLayout.h`:** Defines how many speakers and their positions in space. The Spatializer uses this info to compute distances. This file (or related data) would be used by the Spatializer to get each speaker’s coordinates. Possibly loaded or defined at init (e.g., positions for a 5.1 setup).
- **`mainplayer.cpp`:** The initialization of the audio engine (possibly in mainplayer) will set up the Spatializer. For example, main might create a SpatialRenderer instance, pass it the speaker layout and then register it as the audio callback via the Backend Adapter. The master coordination of starting the audio thread often touches this agent.
- **`SourceBufferManager.cpp`:** (Mentioned in Streaming) – the Spatializer will interface with whatever provides source audio buffers. It might not directly open that file, but it will call into a buffer manager to get the next block for each source each frame.

## Hard Real-Time Constraints

The Spatializer operates under the tightest timing constraints of any agent, as it is part of the audio callback:

- **Deterministic Execution Time:** The processing per frame must complete within the audio buffer interval (e.g., if buffer is 256 samples at 48kHz, ~5.3ms). This execution time should not significantly vary with scene changes (to avoid unpredictability). Algorithms must be chosen to be O(N×M) at worst (N sources, M speakers), and if N or M are large, consider splitting work across frames or limiting N and M.
- **No Locks or Allocations:** Absolutely no mutex locks, sleep calls, or memory allocations/free within the spatializer processing loop. All memory (buffers for each channel, arrays for gains, etc.) should be allocated during initialization. All locks for shared data (like reading positions) should be avoided in this thread – use lock-free structures as set up by the Threading agent (e.g., atomic pointer to positions as discussed in Pose and Control).
- **Real-Time Safe Math:** Use operations that are guaranteed to complete quickly. For example, avoid calling complex library functions within the loop if possible. If trig or sqrt are needed for distance, note that sqrt is usually fast enough on modern CPUs, but if optimization is needed, one could use squared distances with adjusted rolloff to skip sqrt. The key is no unexpected worst-case (e.g., no exception handling thrown per sample or anything of that sort).
- **Memory Access Patterns:** The agent will be reading source audio buffers and writing to output buffers intensively. Ensure memory access is cache-friendly (contiguous arrays, avoid scattered random access). For instance, interleaved vs planar: It’s often easier to have planar audio data (separate buffer per channel) for mixing; the Spatializer can accumulate into an array per speaker. If the output needs interleaved format, convert at the very end (possibly in Output Remap or Backend). Similarly, reading sources sequentially is good – perhaps each source’s buffer is contiguous float samples.
- **Inline Optimizations:** If using C++ and templates, consider inlining the small functions (like computing gain) to avoid function call overhead in the innermost loop. Also consider fixed-size arrays for small M or N to enable unrolling. However, these are micro-optimizations – profile to see if they matter. The main idea is to keep the inner loop as tight as possible.
- **Testing Under Load:** Plan to test the Spatializer with worst-case scenarios (max number of sources, all moving quickly, etc.) to ensure it still meets timing. Use high-resolution timers or the backend’s xrun counters to verify no overruns. This will likely be documented in RENDERING.md or a testing doc but is mentioned here as a reminder of the real-time promise this agent must keep.

## Interaction with Other Agents

- **Streaming Agent:** The Spatializer pulls audio data from the Streaming agent’s buffers every frame. The interface must allow the Spatializer to get a pointer to the next audio block for each source without blocking. Likely, before mixing, the Spatializer will iterate through each active source, retrieve the buffer (or silence if none available), and then apply spatialization. Coordination is needed so that when a source ends or underruns, the Spatializer handles it (maybe skip or fill with zeros) and potentially informs Streaming or control if needed (though direct communication back might be minimal in the real-time thread).
- **Pose and Control Agent:** For each source (and possibly the listener or speaker layout if dynamic), the Spatializer needs the current position. This agent will access data that Pose and Control maintains. The interface could be direct (like spatializer holds a reference to a read-only scene state struct) or via a method call (e.g., `PoseControl.getPositions()` returns an array of positions). This must be done in a lock-free manner. The Spatializer should ideally grab all needed position data at the start of the frame processing (so it’s using a coherent snapshot for that entire block).
- **Compensation and Gain Agent:** There may be an interplay where the Spatializer uses parameters from the Compensation agent. For example, if Compensation defines a distance attenuation curve, the Spatializer might incorporate that by multiplying an extra factor into each source’s gain before mixing. Alternatively, the Spatializer could simply output raw spatial mix and a separate stage (Compensation agent) adjusts the overall gains per channel or source. The design choice should be consistent: if the Compensation agent is separate in the pipeline (e.g., after mixing), then Spatializer focuses purely on panning. If Compensation is integrated, Spatializer applies those gain adjustments directly. Either way, coordinate to not double-apply or miss apply attenuation. This doc assumes possibly that the Spatializer will integrate basic distance rolloff, but any complex loudness compensation (like equal loudness curves or dynamic range) might be in the Compensation stage.
- **LFE Router Agent:** The Spatializer primarily handles main speakers. However, if a source is designated as LFE-only or if DBAP should ignore the subwoofer (common, since subwoofer is non-localized), the Spatializer might exclude LFE from its panning and let the LFE Router handle that separately. For instance, Spatializer could output M channels for mains and a separate buffer for LFE remains empty unless LFE agent fills it. Interaction: the Spatializer might tag certain content for LFE (e.g., if a source’s position is such that only sub should play it, though usually that’s not position-based). More likely, the LFE Router will tap into Spatializer’s output. Ensure these two don’t conflict: e.g., if Spatializer normalizes gains excluding LFE, the LFE addition later won’t cause clipping.
- **Output Remap Agent:** After Spatializer (and LFE processing), the audio channels will be mapped to hardware. The Spatializer doesn’t need to know about channel reordering; it will output in a consistent internal channel order (say, an order matching the SpeakerLayout definition). The Output Remap agent will then shuffle or mix down as needed. Interaction is minimal other than agreeing on what that internal channel order is. Document the assumed channel order (like [Speaker1, Speaker2, ..., LFE]) so the Output Remap knows how to interpret the buffer.
- **Backend Adapter Agent:** The Spatializer ultimately provides the final mixed buffer (after any LFE and gain compensation) to the Backend (either directly or via Output Remap). If the Backend uses a callback mechanism, the Spatializer’s code _is_ what runs in that callback. For example, the Backend might call `SpatialRenderer::process(float* outputBuffer, unsigned frames)` each time it needs audio. Thus, the Spatializer agent is tightly integrated with the Backend’s operation. They must agree on buffer format (non-interleaved vs interleaved, float vs int samples, etc.). If conversion is needed, either Spatializer does it or the Backend might handle it; typically, we keep Spatializer in floating-point and do conversion at the last moment if required by hardware.
- **Threading and Safety Agent:** All communication patterns (with Streaming, Pose/Control, etc.) used by Spatializer adhere to the thread-safe mechanisms. The Spatializer developer must strictly follow those rules (e.g., only use atomic or lock-free reads of shared data, do not attempt to modify shared state on the audio thread, etc.). Coordination with the Threading agent’s design (like using provided lock-free queue classes) is essential. The Spatializer may be where these designs are put to the ultimate test, since any mistake here can directly cause glitches.

## Data Structures & Interfaces

- **Gain Matrix / Coefficients:** There may be an internal matrix or array `[numSources][numSpeakers]` for gain values. Computing this from scratch every frame might be excessive if nothing changed, so an optimization is to compute when needed:
  - For static or infrequently moving sources, compute once and reuse until movement occurs.
  - Possibly maintain for each source an array of gains for each speaker. Update that array when the source moves (maybe triggered by Pose agent signaling movement).
  - If number of sources is moderate and they move often, computing each frame is simpler and maybe fine. But this structure is worth noting for future optimization.
- **Audio Buffers:** The Spatializer will have an input audio buffer for each source (likely provided by Streaming agent). These could be pointers to external buffers or copied into an internal buffer. And an output buffer for each speaker channel. For example, `float* outBuffers[MAX_SPEAKERS]` where each points to an array of `framesPerBuffer` length. These are allocated once. Each frame, Spatializer zeroes out the output buffers, then for each source, reads its samples and accumulates into outBuffers with the appropriate gain.
- **Speaker Positions:** An array or vector of speaker coordinates (e.g., `std::vector<Vector3> speakerPos`) loaded from the layout config. The Spatializer uses these along with source position to compute distances. If the listener position is considered, it might use relative positions (source minus listener to get distance to each speaker). The math: `distance_i = ||sourcePos - speakerPos_i||`. Possibly stored or computed on the fly. If many sources and speakers, storing some of this might help (like caching sourcePos if it doesn’t change within the buffer).
- **Normalization Factor:** In DBAP, often the gains are set such that sum of squared gains = 1 (to preserve power):contentReference[oaicite:6]{index=6}. That might involve computing a normalization factor per source per frame (or per movement). Implementation might have a small loop to sum up (gain_i^2) and normalize. That’s extra work but necessary for correct DBAP. The structure could hold intermediate sums.
- **Interfaces/Methods:** Likely the Spatializer has a method `processAudio()` which the Backend calls or the audio thread runs. Within it, it might call helper methods like `updateGainsIfNeeded()` and `mixSources()`. It may also interface with the Pose agent by calling something like `sceneState = PoseControl.getSceneState()` at the start, then loop through sceneState.sources. The exact interface will depend on how data is shared (could be as simple as reading global arrays of positions).
- **Support for Different Modes:** While this agent is specifically DBAP, design it such that if in the future other spatialization algorithms (VBAP, Ambisonics, etc.) are added, the code could be extended or swapped. This could mean having an abstract interface for spatializer algorithm and DBAP as one implementation. Not mandatory for now, but a note for architecture.

## Development and Documentation Notes

As the Spatializer (DBAP) Agent is implemented:

- **Precision in Documentation:** Keep this file updated with any deviations in the algorithm or optimizations. For instance, if we decide to approximate distances or use a particular power curve (not exactly inverse-square), record that choice here.
- **Coordinate with Compensation Agent:** Clearly decide which part of distance attenuation is handled here vs in Compensation. Update both docs to reflect a unified approach. E.g., “Spatializer applies 1/r falloff as part of gain; Compensation does further loudness EQ” or vice versa.
- **Update Master Plan:** If there are changes in how the pipeline flows (say we choose to integrate LFE inside the spatialization loop rather than separate), update `realtime_master.md` and relevant agent docs (LFE, Output Remap) to reflect that.
- **Record Performance Metrics:** It’s valuable to note in this document what the expected CPU usage is for given numbers of sources and speakers, once measured. E.g., “Mixing 32 sources to 8 speakers uses ~30% of a core at 48kHz, 256 block”. This helps future maintainers understand the limits. Also record any known bottlenecks or potential future improvements (like “currently uses scalar math, could SIMD optimize if needed”).
- **Testing and Tuning:** Outline how you verified the spatialization. For instance, listened to test tones moving around, or compared output levels to theoretical values. Document any tools or test code in `RENDERING.md` or a test plan. If any discrepancy was found (like slight gain mismatches), note how it was resolved or if it’s acceptable.
- **Interface Clarity:** Ensure that how the Spatializer obtains input and delivers output is clearly documented in the code and summarized here. For example, if using a callback model: “The Backend calls SpatialRenderer::audioCallback, which in turn invokes the Spatializer process; once mixing is done, audioCallback returns the final buffer to the system.”
- **Handoff Considerations:** Future developers might implement different spatialization techniques. Keep the code modular (maybe by isolating DBAP-specific parts) and comment in this doc about where those extension points are (like “Function X computes gains via DBAP – this could be replaced with VBAP or others if needed”). This ensures the documentation is useful beyond just the current implementation.

By maintaining this documentation and rigorously adhering to real-time constraints in implementation, the Spatializer (DBAP) Agent will reliably produce immersive spatial audio, forming the audible centerpiece of the engine.
